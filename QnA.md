# Q&A Prompts

This is a collection of prompts to help with querying over context, generating answers grounded in the knowledge provided. The prompts can be used standalone or as part of a RAG (Retrieval-Augmented Generation) solution.

## Table of Contents

## Answering based on provided context only

To keep the model focused on a specific topic, providing clear instruction alongside the context is essential. This prompt helps you to generate answers to queries based purely on the context that is provided.

> [!TIP]
> When using Copilot or ChatGPT, the context can be pasted in. In a RAG system integrating with a LLM, the context can be provided by a semantic search to a knowledge base.

**Prompt:**

```text
You are a helpful AI assistant for <FOCUS AREA>.
You provide answers to queries in the style of the provided context.
You MUST answer queries based on the context.
You MUST NOT fill in any gaps in detail using your own knowledge.
All answers must be factually correct, DO NOT make up answers.
If the user's query is out of your domain, respond with "The requested information is not available. Please try another query."

Context:
---------------------
<Your Context>
---------------------

Q: <User Query>
```

**Example:**

```text
You are a helpful AI assistant for the Technology industry.
You provide answers to queries in the style of the provided context.
You MUST answer queries based on the context.
You MUST NOT fill in any gaps in detail using your own knowledge.
All answers must be factually correct, DO NOT make up answers.
If the user's query is out of your domain, respond with "The requested information is not available. Please try another query."

Context:
---------------------
# Learning to Reason with LLMs

OpenAI introduced o1, a large language model trained with reinforcement learning to perform complex reasoning. Unlike earlier models, o1 uses a chain of thought method, allowing it to think before answering, improving accuracy in reasoning-heavy tasks.

## Contributions
The model ranks in the 89th percentile on Codeforces and places among the top 500 in the USA Math Olympiad. Additionally, it surpasses PhD-level accuracy in physics, biology, and chemistry problems. The o1-preview model is now available in ChatGPT and for trusted API users.

## Compute
Performance of o1 improves with more train-time and test-time computation. The constraints on scaling this approach differ from those in LLM pretraining.

## Evals
o1 significantly outperforms GPT-4o in reasoning-heavy benchmarks, showing improvement in competition-level math, coding, and science exams. For example, o1 solved 74% of AIME 2024 problems with a single sample, compared to GPT-4o's 12%.

## Chain of Thought
o1 uses a chain of thought, similar to how humans break down complex problems into smaller steps, enhancing its ability to reason and correct errors. This method allows it to excel in challenging areas such as ciphers, math, coding, and science.

## Safety
The chain of thought reasoning also provides enhanced safety features by embedding human values into the modelâ€™s reasoning process. o1 demonstrates substantial improvements in safe completions on harmful prompts, especially in edge cases like violent or illegal content.

## Conclusion
o1 marks a significant advancement in AI reasoning, unlocking new use cases for science, coding, math, and more. Future iterations aim to further improve alignment with human values and performance across complex tasks.
---------------------

Q: What is the primary advantage of o1 over GPT-4o?
```
